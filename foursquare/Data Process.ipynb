{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JgYkSQUfqjJ"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/Kaggle/Foursquare/data/foursquare-location-matching.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taOI1h21gW_f"
      },
      "outputs": [],
      "source": [
        "!pip install -q Levenshtein\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yQztYx7ogRKB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "0a0c0638-c628-438e-d309-5ffdf993ad89"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3374a1c2f010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mLevenshtein\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Levenshtein'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import BallTree\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
        "import pickle\n",
        "import lightgbm as lgbm\n",
        "import Levenshtein\n",
        "import difflib\n",
        "import sklearn\n",
        "import joblib\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "import string\n",
        "from math import radians\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACuh_DoNgVHS"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('./train.csv')\n",
        "test = pd.read_csv('./test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdR8Gv7GghfC"
      },
      "outputs": [],
      "source": [
        "columns = ['id', 'name', 'address', 'city', 'state','zip', 'country', 'url', 'phone', 'categories']\n",
        "for c in columns:\n",
        "  if c != 'id':\n",
        "    train[c] = train[c].astype(str).str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GDcVka2glJ2",
        "outputId": "5ce6ae73-fe00-4eea-db36-d4b99f8896ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0    227763\n",
              "0.0    227763\n",
              "2.0    227762\n",
              "3.0    227762\n",
              "4.0    227762\n",
              "Name: set, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kf = GroupKFold(n_splits=5)\n",
        "for i, (trn_idx, val_idx) in enumerate(kf.split(train, train['point_of_interest'], train['point_of_interest'])):\n",
        "    train.loc[val_idx, \"set\"] = i\n",
        "train[\"set\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY15ymwggm8Z"
      },
      "outputs": [],
      "source": [
        "def add_neighbor_features(df,Neighbors=10,cols=['latitude','longitude']):\n",
        "    print('Start knn grouped by country')\n",
        "    train_df_country = []\n",
        "    # df[cols] = np.deg2rad(df[cols])\n",
        "    for country, country_df in tqdm(df.groupby('country')):\n",
        "        country_df = country_df.reset_index(drop = True)\n",
        "\n",
        "        neighbors = min(len(country_df), Neighbors)\n",
        "        knn = KNeighborsRegressor(n_neighbors = neighbors,\n",
        "                                    metric = 'haversine',\n",
        "                                    n_jobs = -1)\n",
        "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
        "        dists, nears = knn.kneighbors(country_df[['latitude', 'longitude']], \n",
        "                                        return_distance = True)\n",
        "\n",
        "        for k in range(1,neighbors):      \n",
        "            cur_df = country_df[['id']]\n",
        "            cur_df['near_id'] = country_df['id'].values[nears[:, k]]\n",
        "            cur_df['kdist_country'] = dists[:, k]\n",
        "            cur_df['kneighbors_country'] = k\n",
        "            \n",
        "            train_df_country.append(cur_df)\n",
        "    train_df_country = pd.concat(train_df_country)\n",
        "    \n",
        "    print('Start knn')\n",
        "    train_df = []\n",
        "    knn = NearestNeighbors(n_neighbors = Neighbors)\n",
        "    knn.fit(df[['latitude','longitude']], df.index)\n",
        "    dists, nears = knn.kneighbors(df[['latitude','longitude']])\n",
        "    \n",
        "    for k in range(1,Neighbors):            \n",
        "        cur_df = df[['id']]\n",
        "        cur_df['near_id'] = df['id'].values[nears[:, k]]\n",
        "        cur_df['kdist'] = dists[:, k]\n",
        "        cur_df['kneighbors'] = k\n",
        "        train_df.append(cur_df)\n",
        "    \n",
        "    train_df = pd.concat(train_df)\n",
        "    train_df = train_df.merge(train_df_country,\n",
        "    on = ['id', 'near_id'],\n",
        "    how = 'outer')\n",
        "    del train_df_country\n",
        "    \n",
        "    return train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63yMoZxOgozn",
        "outputId": "9176a29f-9559-4980-e6c7-4af06daae56d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start knn grouped by country\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 210/210 [01:07<00:00,  3.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start knn\n",
            "Start knn grouped by country\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 211/211 [01:12<00:00,  2.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start knn\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.concat([\n",
        "    add_neighbor_features(train[train[\"set1\"]==0]), \n",
        "    add_neighbor_features(train[train[\"set1\"]==1]), \n",
        "    add_neighbor_features(train[train[\"set\"]==2]), \n",
        "    add_neighbor_features(train[train[\"set\"]==3]), \n",
        "    add_neighbor_features(train[train[\"set\"]==4]), \n",
        "])\n",
        "train_df = train_df[~train_df['near_id'].isnull()][train_df['id'] != train_df['near_id']].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "gS7TKDC4nY5p",
        "outputId": "83c73ccb-c891-4e98-fce0-fbf42d4e8449"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-164202bf-a17b-48ba-a201-b920707d1470\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>near_id</th>\n",
              "      <th>kdist</th>\n",
              "      <th>kneighbors</th>\n",
              "      <th>kdist_country</th>\n",
              "      <th>kneighbors_country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>E_000001272c6c5d</td>\n",
              "      <td>E_3ef360d8d73dcf</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>E_000002eae2a589</td>\n",
              "      <td>E_c12fb799a8a0e4</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>E_000007f24ebc95</td>\n",
              "      <td>E_b664578fe0e8c2</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>E_000008a8ba4f48</td>\n",
              "      <td>E_c03acb4032c33d</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>E_00001d92066153</td>\n",
              "      <td>E_7e0d8e9138dd56</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10890518</th>\n",
              "      <td>E_4c57a92c895eef</td>\n",
              "      <td>E_1827a3004c29ef</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.126334</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10890519</th>\n",
              "      <td>E_6d33113f66fd27</td>\n",
              "      <td>E_1827a3004c29ef</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.181093</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10890520</th>\n",
              "      <td>E_8c77abc0296481</td>\n",
              "      <td>E_1827a3004c29ef</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.180593</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10890521</th>\n",
              "      <td>E_b639518f5a5a60</td>\n",
              "      <td>E_1827a3004c29ef</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.162739</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10890522</th>\n",
              "      <td>E_f490b8a74a551e</td>\n",
              "      <td>E_1827a3004c29ef</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.163236</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10890523 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-164202bf-a17b-48ba-a201-b920707d1470')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-164202bf-a17b-48ba-a201-b920707d1470 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-164202bf-a17b-48ba-a201-b920707d1470');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                        id           near_id     kdist  kneighbors  \\\n",
              "0         E_000001272c6c5d  E_3ef360d8d73dcf  0.000024         1.0   \n",
              "1         E_000002eae2a589  E_c12fb799a8a0e4  0.000002         1.0   \n",
              "2         E_000007f24ebc95  E_b664578fe0e8c2  0.000005         1.0   \n",
              "3         E_000008a8ba4f48  E_c03acb4032c33d  0.000004         1.0   \n",
              "4         E_00001d92066153  E_7e0d8e9138dd56  0.000002         1.0   \n",
              "...                    ...               ...       ...         ...   \n",
              "10890518  E_4c57a92c895eef  E_1827a3004c29ef       NaN         NaN   \n",
              "10890519  E_6d33113f66fd27  E_1827a3004c29ef       NaN         NaN   \n",
              "10890520  E_8c77abc0296481  E_1827a3004c29ef       NaN         NaN   \n",
              "10890521  E_b639518f5a5a60  E_1827a3004c29ef       NaN         NaN   \n",
              "10890522  E_f490b8a74a551e  E_1827a3004c29ef       NaN         NaN   \n",
              "\n",
              "          kdist_country  kneighbors_country  \n",
              "0              0.000022                 2.0  \n",
              "1              0.000002                 1.0  \n",
              "2              0.000005                 1.0  \n",
              "3              0.000004                 2.0  \n",
              "4              0.000002                 1.0  \n",
              "...                 ...                 ...  \n",
              "10890518       1.126334                 6.0  \n",
              "10890519       1.181093                 6.0  \n",
              "10890520       1.180593                 6.0  \n",
              "10890521       1.162739                 6.0  \n",
              "10890522       1.163236                 6.0  \n",
              "\n",
              "[10890523 rows x 6 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "feat_columns = ['name', 'address', 'city', \n",
        "            'state', 'zip', 'url', \n",
        "           'phone', 'categories', 'country']\n",
        "vec_columns = ['name', 'categories','address','url','phone','country','state']\n",
        "\n",
        "## Train data generated by knn\n",
        "id2index_d = dict(zip(train['id'].values, train.index))\n",
        "\n",
        "\n",
        "tfidf_d = {}\n",
        "for col in vec_columns:\n",
        "  if col == 'categories':\n",
        "    tfidf = TfidfVectorizer(use_idf=False)\n",
        "  else:\n",
        "    tfidf = TfidfVectorizer(ngram_range=(3, 3), analyzer=\"char_wb\", use_idf=False)\n",
        "  tv_fit = tfidf.fit_transform(train[col].fillna(f'no{col}'))\n",
        "  tfidf_d[col] = tv_fit\n",
        "  \n",
        "\n",
        "!pip install -q transformers\n",
        "from transformers import DistilBertModel, DistilBertTokenizer , AutoTokenizer, AutoModel\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pickle\n",
        "\n",
        "def get_vec_col(col,bert_model='distilbert-base-uncased'):\n",
        "    if col == 'text_features':\n",
        "      MAX_LEN = 150\n",
        "    else:\n",
        "      MAX_LEN = 32\n",
        "\n",
        "    class Cat2VecModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Cat2VecModel, self).__init__()\n",
        "            self.distill_bert = AutoModel.from_pretrained(bert_model)\n",
        "            \n",
        "        def forward(self, ids, mask):\n",
        "            x = self.distill_bert(ids, mask)[0]\n",
        "            x = F.normalize((x[:, 1:, :]*mask[:, 1:, None]).mean(axis=1))\n",
        "            return x\n",
        "        \n",
        "    cat2vec_model = Cat2VecModel()\n",
        "    cat2vec_model = cat2vec_model.cuda()\n",
        "        \n",
        "    class InferenceDataset(Dataset):\n",
        "        def __init__(self, df, max_len):\n",
        "            super().__init__()\n",
        "            self.df = df.reset_index(drop=True)\n",
        "            self.max_len = max_len\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(bert_model,do_lower_case=True)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            row = self.df.iloc[index]\n",
        "\n",
        "            inputs = self.tokenizer.encode_plus(\n",
        "                row[col],\n",
        "                None,\n",
        "                add_special_tokens=True,\n",
        "                max_length=MAX_LEN,\n",
        "                padding=\"max_length\",\n",
        "                return_token_type_ids=True,\n",
        "                truncation=True\n",
        "            )\n",
        "            ids = torch.LongTensor(inputs['input_ids'])\n",
        "            mask = torch.LongTensor(inputs['attention_mask'])\n",
        "\n",
        "            return ids, mask\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.df.shape[0]\n",
        "    cat_df = train[[col]].drop_duplicates()\n",
        "    cat_df[col] = cat_df[col].fillna(\"nan\")\n",
        "\n",
        "    cat_ds = InferenceDataset(cat_df, max_len=MAX_LEN)\n",
        "\n",
        "    \n",
        "    import sys\n",
        "    BS = 256\n",
        "    NW = 2    \n",
        "\n",
        "    def inference(ds):\n",
        "        loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
        "                            pin_memory=False, drop_last=False)\n",
        "        tbar = tqdm(loader, file=sys.stdout)\n",
        "\n",
        "        vs = []\n",
        "        with torch.no_grad():\n",
        "            for idx, (ids, masks) in enumerate(tbar):\n",
        "                v = cat2vec_model(ids.cuda(), masks.cuda()).detach().cpu().numpy()\n",
        "                vs.append(v)\n",
        "        return np.concatenate(vs)\n",
        "\n",
        "\n",
        "    V = inference(cat_ds)\n",
        "    \n",
        "    tmp_dict = {}\n",
        "    count = 0\n",
        "    for i in range(len(cat_df)):\n",
        "        tmp_dict[cat_df[col].values[i]] = V[count]\n",
        "        count += 1\n",
        "    np.save(f'/content/drive/MyDrive/Kaggle/Foursquare/data/newdata/{col}_{bert_model}_catvec.npy',tmp_dict)\n",
        "\n",
        "train['text_features'] = train['name'].fillna('nan') + '[sep]' + train['address'].fillna('nan') + '[sep]' + train['categories'].fillna('nan')\n",
        "\n",
        "bert_models = ['distilbert-base-uncased']\n",
        "for col in vec_columns:\n",
        "  for bert_model in bert_models: \n",
        "    get_vec_col(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpHpvMRzgspZ"
      },
      "outputs": [],
      "source": [
        "%load_ext Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm25gFMkguPN"
      },
      "outputs": [],
      "source": [
        "%%cython\n",
        "def LCS(str S, str T):\n",
        "    cdef int i, j\n",
        "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
        "    for i in range(len(S)):\n",
        "        for j in range(len(T)):\n",
        "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
        "    return dp[len(S)][len(T)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HARPwTsV5S8U"
      },
      "outputs": [],
      "source": [
        "def categorical_similarity(A, B):\n",
        "    if not A or not B:\n",
        "        return -1\n",
        "\n",
        "    A = set(str(A).split(\", \"))\n",
        "    B = set(str(B).split(\", \"))\n",
        "\n",
        "    # Find intersection of two sets\n",
        "    nominator = A.intersection(B)\n",
        "\n",
        "    similarity_1 = len(nominator) / len(A)\n",
        "    similarity_2 = len(nominator) / len(B)\n",
        "\n",
        "    return max(similarity_1, similarity_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YKiFhQCgvZb"
      },
      "outputs": [],
      "source": [
        "import Levenshtein\n",
        "import difflib\n",
        "import multiprocessing\n",
        "\n",
        "def add_features(df):   \n",
        "    for col in tqdm(feat_columns):       \n",
        "        if col in vec_columns:\n",
        "            if col != 'text_features':\n",
        "              tv_fit = tfidf_d[col]\n",
        "              indexs = [id2index_d[i] for i in df['id']]\n",
        "              match_indexs = [id2index_d[i] for i in df['near_id']]                    \n",
        "              df[f'{col}_sim'] = tv_fit[indexs].multiply(tv_fit[match_indexs]).sum(axis = 1).A.ravel()\n",
        "       \n",
        "        if col in vec_columns:\n",
        "            for bert_model in bert_models:\n",
        "              tmp_dict = np.load(f'/content/drive/MyDrive/Kaggle/Foursquare/data/newdata/{col}_{bert_model}_catvec.npy',allow_pickle=True).item()\n",
        "              indexs = [id2index_d[i] for i in df['id']]\n",
        "              match_indexs = [id2index_d[i] for i in df['near_id']]   \n",
        "              df[f'{col}_{bert_model}_cat_vec_sim'] = [(tmp_dict[train[col].values[indexs[i]]] * tmp_dict[train[col].values[match_indexs[i]]]).sum() for i in range(len(indexs))]\n",
        "\n",
        "\n",
        "        col_values = train.loc[df['id']][col].values.astype(str)\n",
        "        matcol_values = train.loc[df['near_id']][col].values.astype(str)\n",
        "        \n",
        "        geshs = []\n",
        "        levens = []\n",
        "        jaros = []\n",
        "        lcss = []\n",
        "        ifnull = []\n",
        "        \n",
        "        for s, match_s in zip(col_values, matcol_values):\n",
        "            if s != 'nan' and match_s != 'nan':         \n",
        "                geshs.append(difflib.SequenceMatcher(None, s, match_s).ratio())\n",
        "                levens.append(Levenshtein.distance(s, match_s))\n",
        "                jaros.append(Levenshtein.jaro_winkler(s, match_s))\n",
        "                lcss.append(LCS(str(s), str(match_s)))\n",
        "            else:\n",
        "                geshs.append(np.nan)\n",
        "                levens.append(np.nan)\n",
        "                jaros.append(np.nan)\n",
        "                lcss.append(np.nan)\n",
        "        \n",
        "        df[f'{col}_gesh'] = geshs\n",
        "        df[f'{col}_leven'] = levens\n",
        "        df[f'{col}_jaro'] = jaros\n",
        "        df[f'{col}_lcs'] = lcss\n",
        "        \n",
        "        if col not in ['phone', 'zip']:\n",
        "            df[f'{col}_len'] = list(map(len, col_values))\n",
        "            df[f'match_{col}_len'] = list(map(len, matcol_values)) \n",
        "            df[f'{col}_len_diff'] = np.abs(df[f'{col}_len'] - df[f'match_{col}_len'])\n",
        "            df[f'{col}_nleven'] = df[f'{col}_leven'] / \\\n",
        "                                    df[[f'{col}_len', f'match_{col}_len']].max(axis = 1)\n",
        "            \n",
        "            df[f'{col}_nlcsk'] = df[f'{col}_lcs'] / df[f'match_{col}_len']\n",
        "            df[f'{col}_nlcs'] = df[f'{col}_lcs'] / df[f'{col}_len']\n",
        "            \n",
        "            df = df.drop(f'{col}_len', axis = 1)\n",
        "            df = df.drop(f'match_{col}_len', axis = 1)\n",
        "            gc.collect()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekt-LVUogxQ-"
      },
      "outputs": [],
      "source": [
        "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
        "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
        "\n",
        "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
        "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
        "\n",
        "def get_score(input_df: pd.DataFrame):\n",
        "    scores = []\n",
        "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
        "        targets = poi2ids[id2poi[id_str]]\n",
        "        preds = set(matches.split())\n",
        "        score = len((targets & preds)) / len((targets | preds))\n",
        "        scores.append(score)\n",
        "    scores = np.array(scores)\n",
        "    return scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTg9t_5nS4J5",
        "outputId": "e4ef4058-db67-40ec-e794-ef0655a43eb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "160"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvtQPzYIg9qw",
        "outputId": "c716562a-3b52-4ec4-e86a-d3dcf972ceb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num of unique id: 1138812\n",
            "Num of train data: 10895964\n",
            "Pos rate: 0.068207824475191\n",
            "                               name  latitude  longitude    address  \\\n",
            "id                                                                    \n",
            "E_2a6ddaf1c86eff              月出松公園  0.620037   2.435767  都筑区加賀原1-4   \n",
            "E_55c99c7eb95487             bildel  1.046829   0.409031        nan   \n",
            "E_03ec82952fa8fa  ruang ii/5 fe uii -0.135443   1.927040        nan   \n",
            "E_af8e5bfc9500f3  kwyetiiywpaakhmaa  0.286525   1.765464        nan   \n",
            "E_b3e3eb8ee1f4be  chlidaa plaaephaa  0.239974   1.756095        nan   \n",
            "\n",
            "                                city          state      zip country  url  \\\n",
            "id                                                                          \n",
            "E_2a6ddaf1c86eff      heng bang shi            神奈川県  2240055      jp  nan   \n",
            "E_55c99c7eb95487          tammisaari         nyland    10600      fi  nan   \n",
            "E_03ec82952fa8fa  sleman, yogyakarta      indonesia    55283      id  nan   \n",
            "E_af8e5bfc9500f3   mueang phetchabun     phetchabun      nan      th  nan   \n",
            "E_b3e3eb8ee1f4be            hawhmaak  กรุงเทพมหานคร      nan      th  nan   \n",
            "\n",
            "                 phone          categories point_of_interest  set1  set  \\\n",
            "id                                                                        \n",
            "E_2a6ddaf1c86eff   nan               parks  P_e4abdace17ea8f   1.0  0.0   \n",
            "E_55c99c7eb95487   nan    automotive shops  P_55e3cd69711f91   1.0  1.0   \n",
            "E_03ec82952fa8fa   nan  college classrooms  P_f71a3787638f82   1.0  4.0   \n",
            "E_af8e5bfc9500f3   nan   asian restaurants  P_cc73d41b6a5743   0.0  0.0   \n",
            "E_b3e3eb8ee1f4be   nan    thai restaurants  P_1076074762651c   1.0  2.0   \n",
            "\n",
            "                                                     text_features  \n",
            "id                                                                  \n",
            "E_2a6ddaf1c86eff                     月出松公園[sep]都筑区加賀原1-4[sep]parks  \n",
            "E_55c99c7eb95487               bildel[sep]nan[sep]automotive shops  \n",
            "E_03ec82952fa8fa  ruang ii/5 fe uii[sep]nan[sep]college classrooms  \n",
            "E_af8e5bfc9500f3   kwyetiiywpaakhmaa[sep]nan[sep]asian restaurants  \n",
            "E_b3e3eb8ee1f4be    chlidaa plaaephaa[sep]nan[sep]thai restaurants  \n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "train = train.set_index('id')\n",
        "ids = train_df['id'].tolist()\n",
        "match_ids = train_df['near_id'].tolist()\n",
        "\n",
        "poi = train.loc[ids]['point_of_interest'].values\n",
        "match_poi = train.loc[match_ids]['point_of_interest'].values\n",
        "\n",
        "train_df['label'] = np.array(poi == match_poi, dtype = np.int8)\n",
        "del poi, match_poi, ids, match_ids\n",
        "gc.collect()\n",
        "print('Num of unique id: %s' % train_df['id'].nunique())\n",
        "print('Num of train data: %s' % len(train_df))\n",
        "print('Pos rate: %s' % train_df['label'].mean())\n",
        "print(train.sample(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAwLlyYGdmzz",
        "outputId": "0f81c479-0586-4423-c86d-db8015d84b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique id: 1138812\n",
            "Unique id: 1138812\n",
            "IoU score: 0.8983671494512845\n"
          ]
        }
      ],
      "source": [
        "## Eval\n",
        "data = train.reset_index()\n",
        "\n",
        "id2poi = get_id2poi(data)\n",
        "poi2ids = get_poi2ids(data)\n",
        "\n",
        "eval_df = pd.DataFrame()\n",
        "eval_df['id'] = data['id'].unique().tolist()\n",
        "eval_df['near_id'] = eval_df['id']\n",
        "print('Unique id: %s' % len(eval_df))\n",
        "\n",
        "eval_df_ = train_df[train_df['label'] == 1][['id', 'near_id']]\n",
        "eval_df = pd.concat([eval_df, eval_df_])\n",
        "\n",
        "eval_df = eval_df.groupby('id')['near_id'].apply(list).reset_index()\n",
        "eval_df['matches'] = eval_df['near_id'].apply(lambda x: ' '.join(set(x)))\n",
        "print('Unique id: %s' % len(eval_df))\n",
        "\n",
        "iou_score = get_score(eval_df)\n",
        "print('IoU score: %s' % iou_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASXUefncg_sg",
        "outputId": "b14c61cc-50c1-467f-8257-29e69eb63319"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [16:00<00:00, 106.74s/it]\n",
            "100%|██████████| 9/9 [15:28<00:00, 103.13s/it]\n",
            "100%|██████████| 9/9 [15:18<00:00, 102.02s/it]\n",
            "100%|██████████| 9/9 [15:45<00:00, 105.08s/it]\n",
            "100%|██████████| 9/9 [15:46<00:00, 105.14s/it]\n"
          ]
        }
      ],
      "source": [
        "feat_columns = ['name', 'address', 'city', 'state', 'zip', 'url', 'phone', 'categories', 'country']\n",
        "\n",
        "for i in range(5):\n",
        "    tmp_ids = train[train['set'] == i].index \n",
        "    cur_data = train_df[train_df['id'].isin(tmp_ids)]\n",
        "    cur_data = add_features(cur_data)\n",
        "    cur_data = cur_data.merge(train.reset_index()[['id','city','state','country','categories','latitude','longitude','zip', 'url', 'address','phone']],on='id',how='left')\n",
        "    cur_data = cur_data.merge(train.reset_index()[['id','city','state','country','categories','latitude','longitude','zip', 'url', 'address','phone',]],left_on='near_id',right_on='id',how='left')\n",
        "    cur_data = cur_data.drop(columns=['id_y'])\n",
        "    cur_data.rename(columns={'id_x':'id'},inplace=True)\n",
        "    cur_data.to_csv('/content/drive/MyDrive/Kaggle/Foursquare/data/newdata/train_data_v8_%s.csv' % i, index = False)    \n",
        "    del cur_data\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IlYf5IL714H3"
      },
      "outputs": [],
      "source": [
        " "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Data Process",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}